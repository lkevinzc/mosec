{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Model Serving made Efficient in the Cloud. Introduction \u00a4 Mosec is a high-performance and flexible model serving framework for building ML model-enabled backend and microservices. It bridges the gap between any machine learning models you just trained and the efficient online service API. Highly performant : web layer and task coordination built with Rust \ud83e\udd80, which offers blazing speed in addition to efficient CPU utilization powered by async I/O Ease of use : user interface purely in Python \ud83d\udc0d, by which users can serve their models in an ML framework-agnostic manner using the same code as they do for offline testing Dynamic batching : aggregate requests from different users for batched inference and distribute results back Pipelined stages : spawn multiple processes for pipelined stages to handle CPU/GPU/IO mixed workloads Cloud friendly : designed to run in the cloud, with the model warmup, graceful shutdown, and Prometheus monitoring metrics, easily managed by Kubernetes or any container orchestration systems Do one thing well : focus on the online serving part, users can pay attention to the model performance and business logic Installation \u00a4 Mosec requires Python 3.7 or above. Install the latest PyPI package with: > pip install -U mosec Usage \u00a4 Write the server \u00a4 Import the libraries and set up a basic logger to better observe what happens. import logging from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) Then, we build an API to calculate the exponential with base e for a given number. To achieve that, we simply inherit the Worker class and override the forward method. Note that the input req is by default a JSON-decoded object, e.g., a dictionary here (wishfully it receives data like {\"x\": 1} ). We also enclose the input parsing part with a try...except... block to reject invalid input (e.g., no key named \"x\" or field \"x\" cannot be converted to float ). import math class CalculateExp ( Worker ): def forward ( self , req : dict ) -> dict : try : x = float ( req [ \"x\" ]) except KeyError : raise ValidationError ( \"cannot find key 'x'\" ) except ValueError : raise ValidationError ( \"cannot convert 'x' value to float\" ) y = math . exp ( x ) # f(x) = e ^ x logger . debug ( f \"e ^ { x } = { y } \" ) return { \"y\" : y } Finally, we append the worker to the server to construct a single-stage workflow , and we specify the number of processes we want it to run in parallel. Then we run the server. if __name__ == \"__main__\" : server = Server () server . append_worker ( CalculateExp , num = 2 ) # we spawn two processes for parallel computing server . run () Run the server \u00a4 After merging the snippets above into a file named server.py , we can first have a look at the command line arguments: > python server.py --help Then let's start the server... > python server.py and in another terminal, test it: > curl -X POST http://127.0.0.1:8000/inference -d '{\"x\": 2}' { \"y\": 7.38905609893065 } > curl -X POST http://127.0.0.1:8000/inference -d '{\"input\": 2}' # wrong schema validation error: cannot find key 'x' or check the metrics: > curl http://127.0.0.1:8000/metrics For more debug logs, you can enable it by changing the Python & Rust log level: logger . setLevel ( logging . DEBUG ) > RUST_LOG = debug python server.py That's it! You have just hosted your exponential-computing model as a server! \ud83d\ude09 Example \u00a4 More ready-to-use examples can be found in the Example section. It includes: Multi-stage workflow Batch processing worker Shared memory IPC Customized GPU allocation PyTorch deep learning models: sentiment analysis image recognition Qualitative Comparison * \u00a4 Batcher Pipeline Parallel I/O Format (1) Framework (2) Backend Activity TF Serving \u2705 \u2705 \u2705 Limited (a) Heavily TF C++ Triton \u2705 \u2705 \u2705 Limited Multiple C++ MMS \u2705 \u274c \u2705 Limited Heavily MX Java BentoML \u2705 \u274c \u274c Limited (b) Multiple Python Streamer \u2705 \u274c \u2705 Customizable Agnostic Python Flask (3) \u274c \u274c \u274c Customizable Agnostic Python Mosec \u2705 \u2705 \u2705 Customizable Agnostic Rust *As accessed on 08 Oct 2021. By no means is this comparison showing that other frameworks are inferior, but rather it is used to illustrate the trade-off. The information is not guaranteed to be absolutely accurate. Please let us know if you find anything that may be incorrect. (1) : Data format of the service's request and response. \"Limited\" in the sense that the framework has pre-defined requirements on the format. (2) : Supported machine learning frameworks. \"Heavily\" means the serving framework is designed towards a specific ML framework. Thus it is hard, if not impossible, to adapt to others. \"Multiple\" means the serving framework provides adaptation to several existing ML frameworks. \"Agnostic\" means the serving framework does not necessarily care about the ML framework. Hence it supports all ML frameworks (in Python). (3) : Flask is a representative of general purpose web frameworks to host ML models. Contributing \u00a4 We welcome any kind of contribution. Please give us feedback by raising issues or discussing on Discord . You could also directly contribute your code and pull request!","title":"Overview"},{"location":"#introduction","text":"Mosec is a high-performance and flexible model serving framework for building ML model-enabled backend and microservices. It bridges the gap between any machine learning models you just trained and the efficient online service API. Highly performant : web layer and task coordination built with Rust \ud83e\udd80, which offers blazing speed in addition to efficient CPU utilization powered by async I/O Ease of use : user interface purely in Python \ud83d\udc0d, by which users can serve their models in an ML framework-agnostic manner using the same code as they do for offline testing Dynamic batching : aggregate requests from different users for batched inference and distribute results back Pipelined stages : spawn multiple processes for pipelined stages to handle CPU/GPU/IO mixed workloads Cloud friendly : designed to run in the cloud, with the model warmup, graceful shutdown, and Prometheus monitoring metrics, easily managed by Kubernetes or any container orchestration systems Do one thing well : focus on the online serving part, users can pay attention to the model performance and business logic","title":"Introduction"},{"location":"#installation","text":"Mosec requires Python 3.7 or above. Install the latest PyPI package with: > pip install -U mosec","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#write-the-server","text":"Import the libraries and set up a basic logger to better observe what happens. import logging from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) Then, we build an API to calculate the exponential with base e for a given number. To achieve that, we simply inherit the Worker class and override the forward method. Note that the input req is by default a JSON-decoded object, e.g., a dictionary here (wishfully it receives data like {\"x\": 1} ). We also enclose the input parsing part with a try...except... block to reject invalid input (e.g., no key named \"x\" or field \"x\" cannot be converted to float ). import math class CalculateExp ( Worker ): def forward ( self , req : dict ) -> dict : try : x = float ( req [ \"x\" ]) except KeyError : raise ValidationError ( \"cannot find key 'x'\" ) except ValueError : raise ValidationError ( \"cannot convert 'x' value to float\" ) y = math . exp ( x ) # f(x) = e ^ x logger . debug ( f \"e ^ { x } = { y } \" ) return { \"y\" : y } Finally, we append the worker to the server to construct a single-stage workflow , and we specify the number of processes we want it to run in parallel. Then we run the server. if __name__ == \"__main__\" : server = Server () server . append_worker ( CalculateExp , num = 2 ) # we spawn two processes for parallel computing server . run ()","title":"Write the server"},{"location":"#run-the-server","text":"After merging the snippets above into a file named server.py , we can first have a look at the command line arguments: > python server.py --help Then let's start the server... > python server.py and in another terminal, test it: > curl -X POST http://127.0.0.1:8000/inference -d '{\"x\": 2}' { \"y\": 7.38905609893065 } > curl -X POST http://127.0.0.1:8000/inference -d '{\"input\": 2}' # wrong schema validation error: cannot find key 'x' or check the metrics: > curl http://127.0.0.1:8000/metrics For more debug logs, you can enable it by changing the Python & Rust log level: logger . setLevel ( logging . DEBUG ) > RUST_LOG = debug python server.py That's it! You have just hosted your exponential-computing model as a server! \ud83d\ude09","title":"Run the server"},{"location":"#example","text":"More ready-to-use examples can be found in the Example section. It includes: Multi-stage workflow Batch processing worker Shared memory IPC Customized GPU allocation PyTorch deep learning models: sentiment analysis image recognition","title":"Example"},{"location":"#qualitative-comparison","text":"Batcher Pipeline Parallel I/O Format (1) Framework (2) Backend Activity TF Serving \u2705 \u2705 \u2705 Limited (a) Heavily TF C++ Triton \u2705 \u2705 \u2705 Limited Multiple C++ MMS \u2705 \u274c \u2705 Limited Heavily MX Java BentoML \u2705 \u274c \u274c Limited (b) Multiple Python Streamer \u2705 \u274c \u2705 Customizable Agnostic Python Flask (3) \u274c \u274c \u274c Customizable Agnostic Python Mosec \u2705 \u2705 \u2705 Customizable Agnostic Rust *As accessed on 08 Oct 2021. By no means is this comparison showing that other frameworks are inferior, but rather it is used to illustrate the trade-off. The information is not guaranteed to be absolutely accurate. Please let us know if you find anything that may be incorrect. (1) : Data format of the service's request and response. \"Limited\" in the sense that the framework has pre-defined requirements on the format. (2) : Supported machine learning frameworks. \"Heavily\" means the serving framework is designed towards a specific ML framework. Thus it is hard, if not impossible, to adapt to others. \"Multiple\" means the serving framework provides adaptation to several existing ML frameworks. \"Agnostic\" means the serving framework does not necessarily care about the ML framework. Hence it supports all ML frameworks (in Python). (3) : Flask is a representative of general purpose web frameworks to host ML models.","title":"Qualitative Comparison*"},{"location":"#contributing","text":"We welcome any kind of contribution. Please give us feedback by raising issues or discussing on Discord . You could also directly contribute your code and pull request!","title":"Contributing"},{"location":"argument/","text":"usage: python your_model_server.py [-h] [--path PATH] [--capacity CAPACITY] [--timeout TIMEOUT] [--wait WAIT] [--address ADDRESS] [--port PORT] [--namespace NAMESPACE] Mosec Server Configurations optional arguments: -h, --help show this help message and exit --path PATH Unix Domain Socket address for internal Inter-Process Communication (default: /tmp/mosec) --capacity CAPACITY Capacity of the request queue, beyond which new requests will be rejected with status 429 (default: 1024) --timeout TIMEOUT Service timeout for one request (milliseconds) (default: 3000) --wait WAIT Wait time for the batcher to batch (milliseconds) (default: 10) --address ADDRESS Address of the HTTP service (default: 0.0.0.0) --port PORT Port of the HTTP service (default: 8000) --namespace NAMESPACE Namespace for prometheus metrics (default: mosec_service)","title":"Argument"},{"location":"contributing/","text":"Before contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Pull Request Process \u00a4 After you have forked this repository, you could use make install for the first time to install the local development dependencies; afterward, you may use make dev to build the library when you have made any code changes. Before committing your changes, you can use make format && make lint to ensure the codes follow our style standards. Please add corresponding tests to your change if that's related to new feature or API, and ensure make test can pass. Submit your pull request. Contacts \u00a4 Keming kemingy94@gmail.com zclzc lkevinzc@gmail.com","title":"Contributing"},{"location":"contributing/#pull-request-process","text":"After you have forked this repository, you could use make install for the first time to install the local development dependencies; afterward, you may use make dev to build the library when you have made any code changes. Before committing your changes, you can use make format && make lint to ensure the codes follow our style standards. Please add corresponding tests to your change if that's related to new feature or API, and ensure make test can pass. Submit your pull request.","title":"Pull Request Process"},{"location":"contributing/#contacts","text":"Keming kemingy94@gmail.com zclzc lkevinzc@gmail.com","title":"Contacts"},{"location":"interface/","text":"mosec.server \u00a4 MOSEC server interface. This module provides a way to define the service components for machine learning model serving. mosec.server.Server \u00a4 MOSEC server interface. It allows users to sequentially append workers they implemented, builds the workflow pipeline automatically and starts up the server. Batching \u00a4 The user may enable the batching feature for any stage when the corresponding worker is appended, by setting the max_batch_size . Multiprocess \u00a4 The user may spawn multiple processes for any stage when the corresponding worker is appended, by setting the num . IPC Wrapper \u00a4 The user may wrap the inter-process communication to use shared memory, e.g. pyarrow plasma, by providing the IPC wrapper for the server. Source code in mosec/server.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 class Server : \"\"\"MOSEC server interface. It allows users to sequentially append workers they implemented, builds the workflow pipeline automatically and starts up the server. ###### Batching > The user may enable the batching feature for any stage when the corresponding worker is appended, by setting the `max_batch_size`. ###### Multiprocess > The user may spawn multiple processes for any stage when the corresponding worker is appended, by setting the `num`. ###### IPC Wrapper > The user may wrap the inter-process communication to use shared memory, e.g. pyarrow plasma, by providing the IPC wrapper for the server. \"\"\" # pylint: disable=too-many-instance-attributes def __init__ ( self , ipc_wrapper : Optional [ Union [ IPCWrapper , partial ]] = None , ): \"\"\"Initialize a MOSEC Server. Args: ipc_wrapper (Optional[Union[IPCWrapper, partial]], optional): IPCWrapper class. Defaults to None. \"\"\" self . ipc_wrapper = ipc_wrapper self . _worker_cls : List [ Type [ Worker ]] = [] self . _worker_num : List [ int ] = [] self . _worker_mbs : List [ int ] = [] self . _coordinator_env : List [ Union [ None , List [ Dict [ str , str ]]]] = [] self . _coordinator_ctx : List [ str ] = [] self . _coordinator_pools : List [ List [ Union [ mp . Process , None ]]] = [] self . _coordinator_shutdown : Event = mp . get_context ( \"spawn\" ) . Event () self . _coordinator_shutdown_notify : Event = mp . get_context ( \"spawn\" ) . Event () self . _controller_process : Optional [ subprocess . Popen ] = None self . _daemon : Dict [ str , Union [ subprocess . Popen , mp . Process ]] = {} self . _configs : dict = {} self . _server_shutdown : bool = False signal . signal ( signal . SIGTERM , self . _terminate ) signal . signal ( signal . SIGINT , self . _terminate ) def _validate_server ( self ): assert len ( self . _worker_cls ) > 0 , ( \"no worker registered \\n \" \"help: use `.append_worker(...)` to register at least one worker\" ) @staticmethod def _validate_arguments ( worker , num , max_batch_size , start_method , env , ): def validate_int_ge_1 ( number , name ): assert isinstance ( number , int ), f \" { name } must be integer but you give { type ( number ) } \" assert number >= 1 , f \" { name } must be no less than 1\" def validate_env (): if env is None : return def validate_str_dict ( dictionary : Dict ): for key , value in dictionary . items (): if not ( isinstance ( key , str ) and isinstance ( value , str )): return False return True assert len ( env ) == num , \"len(env) must equal to num\" valid = True if not isinstance ( env , List ): valid = False elif not all ( isinstance ( x , Dict ) and validate_str_dict ( x ) for x in env ): valid = False assert valid , \"env must be a list of string dictionary\" validate_env () assert issubclass ( worker , Worker ), \"worker must be inherited from mosec.Worker\" validate_int_ge_1 ( num , \"worker number\" ) validate_int_ge_1 ( max_batch_size , \"maximum batch size\" ) assert ( start_method in NEW_PROCESS_METHOD ), f \"start method must be one of { NEW_PROCESS_METHOD } \" def _check_daemon ( self ): for name , proc in self . _daemon . items (): if proc is not None : terminate = False if isinstance ( proc , mp . Process ): code = proc . exitcode elif isinstance ( proc , subprocess . Popen ): code = proc . poll () if code : terminate = True if terminate : self . _terminate ( code , f \"mosec daemon [ { name } ] exited on error code: { code } \" , ) def _controller_args ( self ): args = [] for key , value in self . _configs . items (): args . extend ([ f \"-- { key } \" , str ( value )]) for batch_size in self . _worker_mbs : args . extend ([ \"--batches\" , str ( batch_size )]) logger . info ( \"Mosec Server Configurations: %s \" , args ) return args def _start_controller ( self ): \"\"\"Subprocess to start controller program.\"\"\" self . _configs = vars ( parse_arguments ()) if not self . _server_shutdown : path = self . _configs [ \"path\" ] if exists ( path ): logger . info ( \"path already exists, try to remove it: %s \" , path ) rmtree ( path ) path = Path ( pkg_resources . resource_filename ( \"mosec\" , \"bin\" ), \"mosec\" ) # pylint: disable=consider-using-with self . _controller_process = subprocess . Popen ( [ path ] + self . _controller_args () ) self . register_daemon ( \"controller\" , self . _controller_process ) def _terminate ( self , signum , framestack ): logger . info ( \"[ %s ] terminating server [ %s ] ...\" , signum , framestack ) self . _server_shutdown = True @staticmethod def _clean_pools ( processes : List [ Union [ mp . Process , None ]], ) -> List [ Union [ mp . Process , None ]]: for i , process in enumerate ( processes ): if process is None or process . exitcode is not None : processes [ i ] = None return processes def _manage_coordinators ( self ): first = True while not self . _server_shutdown : for stage_id , ( w_cls , w_num , w_mbs , c_ctx , c_env ) in enumerate ( zip ( self . _worker_cls , self . _worker_num , self . _worker_mbs , self . _coordinator_ctx , self . _coordinator_env , ) ): # for every sequential stage self . _coordinator_pools [ stage_id ] = self . _clean_pools ( self . _coordinator_pools [ stage_id ] ) if all ( self . _coordinator_pools [ stage_id ]): # this stage is healthy continue if not first and not any ( self . _coordinator_pools [ stage_id ]): # this stage might contain bugs self . _terminate ( 1 , f \"all workers at stage { stage_id } exited;\" \" please check for bugs or socket connection issues\" , ) break stage = \"\" if stage_id == 0 : stage += STAGE_INGRESS if stage_id == len ( self . _worker_cls ) - 1 : stage += STAGE_EGRESS for worker_id in range ( w_num ): # for every worker in each stage if self . _coordinator_pools [ stage_id ][ worker_id ] is not None : continue coordinator_process = mp . get_context ( c_ctx ) . Process ( target = Coordinator , args = ( w_cls , w_mbs , stage , self . _coordinator_shutdown , self . _coordinator_shutdown_notify , self . _configs [ \"path\" ], stage_id + 1 , worker_id + 1 , self . ipc_wrapper , ), daemon = True , ) with env_var_context ( c_env , worker_id ): coordinator_process . start () self . _coordinator_pools [ stage_id ][ worker_id ] = coordinator_process first = False self . _check_daemon () sleep ( GUARD_CHECK_INTERVAL ) def _halt ( self ): \"\"\"Graceful shutdown.\"\"\" # notify coordinators for the shutdown self . _coordinator_shutdown_notify . set () # terminate controller first and wait for a graceful period if self . _controller_process : self . _controller_process . terminate () graceful_period = monotonic () + self . _configs [ \"timeout\" ] / 1000 while monotonic () < graceful_period : ctr_exitcode = self . _controller_process . poll () if ctr_exitcode is not None : # exited if ctr_exitcode : # on error logger . error ( \"mosec controller halted on error: %d \" , ctr_exitcode ) else : logger . info ( \"mosec controller halted normally\" ) break sleep ( 0.1 ) # shutdown coordinators self . _coordinator_shutdown . set () logger . info ( \"mosec server exited. see you.\" ) def register_daemon ( self , name : str , proc : mp . Process ): \"\"\"Register a daemon to be monitored. Args: name (str): the name of this daemon proc (mp.Process): the process handle of the daemon \"\"\" assert isinstance ( name , str ), \"daemon name should be a string\" assert isinstance ( proc , ( mp . Process , subprocess . Popen ) ), f \" { type ( proc ) } is not a process or subprocess\" self . _daemon [ name ] = proc def append_worker ( self , worker : Type [ Worker ], num : int = 1 , max_batch_size : int = 1 , start_method : str = \"spawn\" , env : Union [ None , List [ Dict [ str , str ]]] = None , ): \"\"\"Sequentially appends workers to the workflow pipeline. Arguments: worker: the class you inherit from `Worker` which implements the `forward` method num: the number of processes for parallel computing (>=1) max_batch_size: the maximum batch size allowed (>=1) start_method: the process starting method (\"spawn\" or \"fork\") env: the environment variables to set before starting the process \"\"\" self . _validate_arguments ( worker , num , max_batch_size , start_method , env ) self . _worker_cls . append ( worker ) self . _worker_num . append ( num ) self . _worker_mbs . append ( max_batch_size ) self . _coordinator_env . append ( env ) self . _coordinator_ctx . append ( start_method ) self . _coordinator_pools . append ([ None ] * num ) def run ( self ): \"\"\"Start the mosec model server.\"\"\" self . _validate_server () self . _start_controller () try : self . _manage_coordinators () # pylint: disable=broad-except except Exception : logger . error ( traceback . format_exc () . replace ( \" \\n \" , \" \" )) self . _halt () mosec . server . Server . __init__ ( ipc_wrapper : Optional [ Union [ IPCWrapper , partial ]] = None ) \u00a4 Initialize a MOSEC Server. Parameters: Name Type Description Default ipc_wrapper Optional [ Union [ IPCWrapper , partial ]] IPCWrapper class. Defaults to None. None Source code in mosec/server.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , ipc_wrapper : Optional [ Union [ IPCWrapper , partial ]] = None , ): \"\"\"Initialize a MOSEC Server. Args: ipc_wrapper (Optional[Union[IPCWrapper, partial]], optional): IPCWrapper class. Defaults to None. \"\"\" self . ipc_wrapper = ipc_wrapper self . _worker_cls : List [ Type [ Worker ]] = [] self . _worker_num : List [ int ] = [] self . _worker_mbs : List [ int ] = [] self . _coordinator_env : List [ Union [ None , List [ Dict [ str , str ]]]] = [] self . _coordinator_ctx : List [ str ] = [] self . _coordinator_pools : List [ List [ Union [ mp . Process , None ]]] = [] self . _coordinator_shutdown : Event = mp . get_context ( \"spawn\" ) . Event () self . _coordinator_shutdown_notify : Event = mp . get_context ( \"spawn\" ) . Event () self . _controller_process : Optional [ subprocess . Popen ] = None self . _daemon : Dict [ str , Union [ subprocess . Popen , mp . Process ]] = {} self . _configs : dict = {} self . _server_shutdown : bool = False signal . signal ( signal . SIGTERM , self . _terminate ) signal . signal ( signal . SIGINT , self . _terminate ) mosec . server . Server . append_worker ( worker : Type [ Worker ], num : int = 1 , max_batch_size : int = 1 , start_method : str = 'spawn' , env : Union [ None , List [ Dict [ str , str ]]] = None ) \u00a4 Sequentially appends workers to the workflow pipeline. Parameters: Name Type Description Default worker Type [ Worker ] the class you inherit from Worker which implements the forward method required num int the number of processes for parallel computing (>=1) 1 max_batch_size int the maximum batch size allowed (>=1) 1 start_method str the process starting method (\"spawn\" or \"fork\") 'spawn' env Union [None, List [ Dict [ str , str ]]] the environment variables to set before starting the process None Source code in mosec/server.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def append_worker ( self , worker : Type [ Worker ], num : int = 1 , max_batch_size : int = 1 , start_method : str = \"spawn\" , env : Union [ None , List [ Dict [ str , str ]]] = None , ): \"\"\"Sequentially appends workers to the workflow pipeline. Arguments: worker: the class you inherit from `Worker` which implements the `forward` method num: the number of processes for parallel computing (>=1) max_batch_size: the maximum batch size allowed (>=1) start_method: the process starting method (\"spawn\" or \"fork\") env: the environment variables to set before starting the process \"\"\" self . _validate_arguments ( worker , num , max_batch_size , start_method , env ) self . _worker_cls . append ( worker ) self . _worker_num . append ( num ) self . _worker_mbs . append ( max_batch_size ) self . _coordinator_env . append ( env ) self . _coordinator_ctx . append ( start_method ) self . _coordinator_pools . append ([ None ] * num ) mosec . server . Server . register_daemon ( name : str , proc : mp . Process ) \u00a4 Register a daemon to be monitored. Parameters: Name Type Description Default name str the name of this daemon required proc mp . Process the process handle of the daemon required Source code in mosec/server.py 293 294 295 296 297 298 299 300 301 302 303 304 def register_daemon ( self , name : str , proc : mp . Process ): \"\"\"Register a daemon to be monitored. Args: name (str): the name of this daemon proc (mp.Process): the process handle of the daemon \"\"\" assert isinstance ( name , str ), \"daemon name should be a string\" assert isinstance ( proc , ( mp . Process , subprocess . Popen ) ), f \" { type ( proc ) } is not a process or subprocess\" self . _daemon [ name ] = proc mosec . server . Server . run () \u00a4 Start the mosec model server. Source code in mosec/server.py 332 333 334 335 336 337 338 339 340 341 def run ( self ): \"\"\"Start the mosec model server.\"\"\" self . _validate_server () self . _start_controller () try : self . _manage_coordinators () # pylint: disable=broad-except except Exception : logger . error ( traceback . format_exc () . replace ( \" \\n \" , \" \" )) self . _halt () mosec . server . env_var_context ( env : Union [ None , List [ Dict [ str , str ]]], index : int ) \u00a4 Manage the environment variables for a worker process. Source code in mosec/server.py 344 345 346 347 348 349 350 351 352 353 354 355 356 @contextlib . contextmanager def env_var_context ( env : Union [ None , List [ Dict [ str , str ]]], index : int ): \"\"\"Manage the environment variables for a worker process.\"\"\" default : Dict = {} try : if env is not None : for key , value in env [ index ] . items (): default [ key ] = os . getenv ( key , \"\" ) os . environ [ key ] = value yield None finally : for key , value in default . items (): os . environ [ key ] = value mosec.worker \u00a4 MOSEC worker interface. This module provides the interface to define a worker with such behaviors initialize serialize/deserialize data to/from another worker serialize/deserialize data to/from the client side data processing mosec.worker.Worker \u00a4 Bases: abc . ABC MOSEC worker interface. It provides default IPC (de)serialization methods, stores the worker meta data including its stage and maximum batch size, and leaves the forward method to be implemented by the users. By default, we use JSON encoding. But users are free to customize via simply overridding the deserialize method in the first stage (we term it as ingress stage) and/or the serialize method in the last stage (we term it as egress stage). For the encoding customization, there are many choices including MessagePack , Protocol Buffer and many other out-of-the-box protocols. Users can even define their own protocol and use it to manipulate the raw bytes! A naive customization can be found in this example . Note \u00a4 The \" same type \" mentioned below is applicable only when the stage disables batching. For a stage that enables batching , the worker 's forward should accept a list and output a list, where each element will follow the \" same type \" constraint. Source code in mosec/worker.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class Worker ( abc . ABC ): \"\"\"MOSEC worker interface. It provides default IPC (de)serialization methods, stores the worker meta data including its stage and maximum batch size, and leaves the `forward` method to be implemented by the users. By default, we use [JSON](https://www.json.org/) encoding. But users are free to customize via simply overridding the `deserialize` method in the **first** stage (we term it as _ingress_ stage) and/or the `serialize` method in the **last** stage (we term it as _egress_ stage). For the encoding customization, there are many choices including [MessagePack](https://msgpack.org/index.html), [Protocol Buffer](https://developers.google.com/protocol-buffers) and many other out-of-the-box protocols. Users can even define their own protocol and use it to manipulate the raw bytes! A naive customization can be found in [this example](https://mosecorg.github.io/mosec/example/pytorch/#sentiment-analysis). ###### Note > The \"_same type_\" mentioned below is applicable only when the stage disables batching. For a stage that [enables batching][mosec.server.Server--batching], the `worker`'s `forward` should accept a list and output a list, where each element will follow the \"_same type_\" constraint. \"\"\" # pylint: disable=no-self-use example : Any = None _worker_id : int = 0 def __init__ ( self ): \"\"\"Initialize the worker.\"\"\" self . _stage : str = \"\" self . _max_batch_size : int = 1 def serialize_ipc ( self , data ) -> bytes : \"\"\"Define IPC serialize method.\"\"\" return pickle . dumps ( data , protocol = pickle . HIGHEST_PROTOCOL ) def deserialize_ipc ( self , data ) -> Any : \"\"\"Define IPC deserialize method.\"\"\" return pickle . loads ( data ) @property def stage ( self ) -> str : \"\"\"Return the stage name.\"\"\" return self . _stage @stage . setter def stage ( self , stage ): self . _stage = stage @property def max_batch_size ( self ) -> int : \"\"\"Return the maximum batch size.\"\"\" return self . _max_batch_size @max_batch_size . setter def max_batch_size ( self , max_batch_size ): self . _max_batch_size = max_batch_size @property def worker_id ( self ) -> int : \"\"\"Return the id of this worker instance. This property returns the worker id in the range of [1, ... ,`num`] (`num` as defined [here][mosec.server.Server--multiprocess]) to differentiate workers in the same stage. \"\"\" return self . _worker_id def serialize ( self , data : Any ) -> bytes : \"\"\"Serialize method for the last stage (egress). No need to override this method by default, but overridable. Arguments: data: the [_*same type_][mosec.worker.Worker--note] as the output of the `forward` you implement Returns: the bytes you want to put into the response body Raises: ValueError: if the data cannot be serialized with JSON \"\"\" try : data_bytes = json . dumps ( data , indent = 2 ) . encode () except Exception as err : raise ValueError from err return data_bytes def deserialize ( self , data : bytes ) -> Any : \"\"\"Deserialize method for the first stage (ingress). No need to override this method by default, but overridable. Arguments: data: the raw bytes extracted from the request body Returns: the [_*same type_][mosec.worker.Worker--note] as the argument of the `forward` you implement Raises: DecodingError: if the data cannot be deserialized with JSON \"\"\" try : data_json = json . loads ( data ) if data else {} except Exception as err : raise DecodingError from err return data_json @abc . abstractmethod def forward ( self , data : Any ) -> Any : \"\"\"Model inference, data processing or computation logic. __Must be overridden__ by the subclass. The implementation should make sure: - (for a single-stage worker) - both the input and output follow the [_*same type_][mosec.worker.Worker--note] rule. - (for a multi-stage worker) - the input of the _ingress_ stage and the output of the _egress_ stage follow the [_*same type_][mosec.worker.Worker--note], while others should align with its adjacent stage's in/output. If any code in the `forward` needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the `__init__` method. \"\"\" raise NotImplementedError mosec . worker . Worker . __init__ () \u00a4 Initialize the worker. Source code in mosec/worker.py 69 70 71 72 def __init__ ( self ): \"\"\"Initialize the worker.\"\"\" self . _stage : str = \"\" self . _max_batch_size : int = 1 mosec . worker . Worker . deserialize ( data : bytes ) -> Any \u00a4 Deserialize method for the first stage (ingress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data bytes the raw bytes extracted from the request body required Returns: Type Description Any the *same type as the argument of Any the forward you implement Raises: Type Description DecodingError if the data cannot be deserialized with JSON Source code in mosec/worker.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def deserialize ( self , data : bytes ) -> Any : \"\"\"Deserialize method for the first stage (ingress). No need to override this method by default, but overridable. Arguments: data: the raw bytes extracted from the request body Returns: the [_*same type_][mosec.worker.Worker--note] as the argument of the `forward` you implement Raises: DecodingError: if the data cannot be deserialized with JSON \"\"\" try : data_json = json . loads ( data ) if data else {} except Exception as err : raise DecodingError from err return data_json mosec . worker . Worker . deserialize_ipc ( data ) -> Any \u00a4 Define IPC deserialize method. Source code in mosec/worker.py 78 79 80 def deserialize_ipc ( self , data ) -> Any : \"\"\"Define IPC deserialize method.\"\"\" return pickle . loads ( data ) mosec . worker . Worker . forward ( data : Any ) -> Any abstractmethod \u00a4 Model inference, data processing or computation logic. Must be overridden by the subclass. The implementation should make sure: (for a single-stage worker) both the input and output follow the *same type rule. (for a multi-stage worker) the input of the ingress stage and the output of the egress stage follow the *same type , while others should align with its adjacent stage's in/output. If any code in the forward needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the __init__ method. Source code in mosec/worker.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @abc . abstractmethod def forward ( self , data : Any ) -> Any : \"\"\"Model inference, data processing or computation logic. __Must be overridden__ by the subclass. The implementation should make sure: - (for a single-stage worker) - both the input and output follow the [_*same type_][mosec.worker.Worker--note] rule. - (for a multi-stage worker) - the input of the _ingress_ stage and the output of the _egress_ stage follow the [_*same type_][mosec.worker.Worker--note], while others should align with its adjacent stage's in/output. If any code in the `forward` needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the `__init__` method. \"\"\" raise NotImplementedError mosec . worker . Worker . max_batch_size () -> int writable property \u00a4 Return the maximum batch size. Source code in mosec/worker.py 91 92 93 94 @property def max_batch_size ( self ) -> int : \"\"\"Return the maximum batch size.\"\"\" return self . _max_batch_size mosec . worker . Worker . serialize ( data : Any ) -> bytes \u00a4 Serialize method for the last stage (egress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data Any the *same type as the output of the forward you implement required Returns: Type Description bytes the bytes you want to put into the response body Raises: Type Description ValueError if the data cannot be serialized with JSON Source code in mosec/worker.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def serialize ( self , data : Any ) -> bytes : \"\"\"Serialize method for the last stage (egress). No need to override this method by default, but overridable. Arguments: data: the [_*same type_][mosec.worker.Worker--note] as the output of the `forward` you implement Returns: the bytes you want to put into the response body Raises: ValueError: if the data cannot be serialized with JSON \"\"\" try : data_bytes = json . dumps ( data , indent = 2 ) . encode () except Exception as err : raise ValueError from err return data_bytes mosec . worker . Worker . serialize_ipc ( data ) -> bytes \u00a4 Define IPC serialize method. Source code in mosec/worker.py 74 75 76 def serialize_ipc ( self , data ) -> bytes : \"\"\"Define IPC serialize method.\"\"\" return pickle . dumps ( data , protocol = pickle . HIGHEST_PROTOCOL ) mosec . worker . Worker . stage () -> str writable property \u00a4 Return the stage name. Source code in mosec/worker.py 82 83 84 85 @property def stage ( self ) -> str : \"\"\"Return the stage name.\"\"\" return self . _stage mosec . worker . Worker . worker_id () -> int property \u00a4 Return the id of this worker instance. This property returns the worker id in the range of [1, ... , num ] ( num as defined here ) to differentiate workers in the same stage. Source code in mosec/worker.py 100 101 102 103 104 105 106 107 108 @property def worker_id ( self ) -> int : \"\"\"Return the id of this worker instance. This property returns the worker id in the range of [1, ... ,`num`] (`num` as defined [here][mosec.server.Server--multiprocess]) to differentiate workers in the same stage. \"\"\" return self . _worker_id mosec.errors \u00a4 Exceptions used in the Worker. Suppose the input dataflow of our model server is as follows: bytes --- deserialize (decoding) ---> data --- parse (validation) ---> valid data If the raw bytes cannot be successfully deserialized, the DecodingError is raised; if the decoded data cannot pass the validation check (usually implemented by users), the ValidationError should be raised. mosec.errors.DecodingError \u00a4 Bases: Exception De-serialization error. The DecodingError should be raised in user-implemented codes when the de-serialization for the request bytes fails. This error will set the status code to HTTP 400 in the response. Source code in mosec/errors.py 28 29 30 31 32 33 34 35 36 class DecodingError ( Exception ): \"\"\"De-serialization error. The `DecodingError` should be raised in user-implemented codes when the de-serialization for the request bytes fails. This error will set the status code to [HTTP 400](\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400) in the response. \"\"\" mosec.errors.ValidationError \u00a4 Bases: Exception Request data validation error. The ValidationError should be raised in user-implemented codes, where the validation for the input data fails. Usually, it should be put after the data de-serialization, which converts the raw bytes into structured data. This error will set the status code to HTTP 422 in the response. Source code in mosec/errors.py 39 40 41 42 43 44 45 46 47 48 class ValidationError ( Exception ): \"\"\"Request data validation error. The `ValidationError` should be raised in user-implemented codes, where the validation for the input data fails. Usually, it should be put after the data de-serialization, which converts the raw bytes into structured data. This error will set the status code to [HTTP 422](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422) in the response. \"\"\" mosec.plugins \u00a4 Provide useful tools to extend MOSEC. mosec.plugins.PlasmaShmWrapper \u00a4 Bases: IPCWrapper Shared memory wrapper using pyarrow Plasma. This public class is an example implementation of the IPCWrapper . It utilizes pyarrow.plasma as the in-memory object store for potentially more efficient data transfer. Source code in mosec/plugins/plasma_shm.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class PlasmaShmWrapper ( IPCWrapper ): \"\"\"Shared memory wrapper using `pyarrow` Plasma. This public class is an example implementation of the `IPCWrapper`. It utilizes `pyarrow.plasma` as the in-memory object store for potentially more efficient data transfer. \"\"\" def __init__ ( self , shm_path : str ) -> None : \"\"\"Initialize the IPC Wrapper as a plasma client. Args: shm_path (str): path of the plasma server. \"\"\" self . client = plasma . connect ( shm_path ) def _put_plasma ( self , data : List [ bytes ]) -> List [ plasma . ObjectID ]: \"\"\"Batch put into plasma memory store.\"\"\" return [ self . client . put ( x ) for x in data ] def _get_plasma ( self , object_ids : List [ plasma . ObjectID ]) -> List [ bytes ]: \"\"\"Batch get from plasma memory store.\"\"\" objects = self . client . get ( object_ids ) self . client . delete ( object_ids ) return objects def put ( self , data : List [ bytes ]) -> List [ bytes ]: \"\"\"Save data to the plasma memory store and return the ID.\"\"\" object_ids = self . _put_plasma ( data ) return [ id . binary () for id in object_ids ] def get ( self , ids : List [ bytes ]) -> List [ bytes ]: \"\"\"Get data from the plasma memory store by ID.\"\"\" object_ids = [ plasma . ObjectID ( id ) for id in ids ] return self . _get_plasma ( object_ids ) mosec . plugins . plasma_shm . PlasmaShmWrapper . __init__ ( shm_path : str ) -> None \u00a4 Initialize the IPC Wrapper as a plasma client. Parameters: Name Type Description Default shm_path str path of the plasma server. required Source code in mosec/plugins/plasma_shm.py 47 48 49 50 51 52 53 def __init__ ( self , shm_path : str ) -> None : \"\"\"Initialize the IPC Wrapper as a plasma client. Args: shm_path (str): path of the plasma server. \"\"\" self . client = plasma . connect ( shm_path ) mosec . plugins . plasma_shm . PlasmaShmWrapper . get ( ids : List [ bytes ]) -> List [ bytes ] \u00a4 Get data from the plasma memory store by ID. Source code in mosec/plugins/plasma_shm.py 70 71 72 73 def get ( self , ids : List [ bytes ]) -> List [ bytes ]: \"\"\"Get data from the plasma memory store by ID.\"\"\" object_ids = [ plasma . ObjectID ( id ) for id in ids ] return self . _get_plasma ( object_ids ) mosec . plugins . plasma_shm . PlasmaShmWrapper . put ( data : List [ bytes ]) -> List [ bytes ] \u00a4 Save data to the plasma memory store and return the ID. Source code in mosec/plugins/plasma_shm.py 65 66 67 68 def put ( self , data : List [ bytes ]) -> List [ bytes ]: \"\"\"Save data to the plasma memory store and return the ID.\"\"\" object_ids = self . _put_plasma ( data ) return [ id . binary () for id in object_ids ]","title":"Interface"},{"location":"interface/#mosec.server","text":"MOSEC server interface. This module provides a way to define the service components for machine learning model serving.","title":"server"},{"location":"interface/#mosec.server.Server","text":"MOSEC server interface. It allows users to sequentially append workers they implemented, builds the workflow pipeline automatically and starts up the server.","title":"Server"},{"location":"interface/#mosec.server.Server--batching","text":"The user may enable the batching feature for any stage when the corresponding worker is appended, by setting the max_batch_size .","title":"Batching"},{"location":"interface/#mosec.server.Server--multiprocess","text":"The user may spawn multiple processes for any stage when the corresponding worker is appended, by setting the num .","title":"Multiprocess"},{"location":"interface/#mosec.server.Server--ipc-wrapper","text":"The user may wrap the inter-process communication to use shared memory, e.g. pyarrow plasma, by providing the IPC wrapper for the server. Source code in mosec/server.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 class Server : \"\"\"MOSEC server interface. It allows users to sequentially append workers they implemented, builds the workflow pipeline automatically and starts up the server. ###### Batching > The user may enable the batching feature for any stage when the corresponding worker is appended, by setting the `max_batch_size`. ###### Multiprocess > The user may spawn multiple processes for any stage when the corresponding worker is appended, by setting the `num`. ###### IPC Wrapper > The user may wrap the inter-process communication to use shared memory, e.g. pyarrow plasma, by providing the IPC wrapper for the server. \"\"\" # pylint: disable=too-many-instance-attributes def __init__ ( self , ipc_wrapper : Optional [ Union [ IPCWrapper , partial ]] = None , ): \"\"\"Initialize a MOSEC Server. Args: ipc_wrapper (Optional[Union[IPCWrapper, partial]], optional): IPCWrapper class. Defaults to None. \"\"\" self . ipc_wrapper = ipc_wrapper self . _worker_cls : List [ Type [ Worker ]] = [] self . _worker_num : List [ int ] = [] self . _worker_mbs : List [ int ] = [] self . _coordinator_env : List [ Union [ None , List [ Dict [ str , str ]]]] = [] self . _coordinator_ctx : List [ str ] = [] self . _coordinator_pools : List [ List [ Union [ mp . Process , None ]]] = [] self . _coordinator_shutdown : Event = mp . get_context ( \"spawn\" ) . Event () self . _coordinator_shutdown_notify : Event = mp . get_context ( \"spawn\" ) . Event () self . _controller_process : Optional [ subprocess . Popen ] = None self . _daemon : Dict [ str , Union [ subprocess . Popen , mp . Process ]] = {} self . _configs : dict = {} self . _server_shutdown : bool = False signal . signal ( signal . SIGTERM , self . _terminate ) signal . signal ( signal . SIGINT , self . _terminate ) def _validate_server ( self ): assert len ( self . _worker_cls ) > 0 , ( \"no worker registered \\n \" \"help: use `.append_worker(...)` to register at least one worker\" ) @staticmethod def _validate_arguments ( worker , num , max_batch_size , start_method , env , ): def validate_int_ge_1 ( number , name ): assert isinstance ( number , int ), f \" { name } must be integer but you give { type ( number ) } \" assert number >= 1 , f \" { name } must be no less than 1\" def validate_env (): if env is None : return def validate_str_dict ( dictionary : Dict ): for key , value in dictionary . items (): if not ( isinstance ( key , str ) and isinstance ( value , str )): return False return True assert len ( env ) == num , \"len(env) must equal to num\" valid = True if not isinstance ( env , List ): valid = False elif not all ( isinstance ( x , Dict ) and validate_str_dict ( x ) for x in env ): valid = False assert valid , \"env must be a list of string dictionary\" validate_env () assert issubclass ( worker , Worker ), \"worker must be inherited from mosec.Worker\" validate_int_ge_1 ( num , \"worker number\" ) validate_int_ge_1 ( max_batch_size , \"maximum batch size\" ) assert ( start_method in NEW_PROCESS_METHOD ), f \"start method must be one of { NEW_PROCESS_METHOD } \" def _check_daemon ( self ): for name , proc in self . _daemon . items (): if proc is not None : terminate = False if isinstance ( proc , mp . Process ): code = proc . exitcode elif isinstance ( proc , subprocess . Popen ): code = proc . poll () if code : terminate = True if terminate : self . _terminate ( code , f \"mosec daemon [ { name } ] exited on error code: { code } \" , ) def _controller_args ( self ): args = [] for key , value in self . _configs . items (): args . extend ([ f \"-- { key } \" , str ( value )]) for batch_size in self . _worker_mbs : args . extend ([ \"--batches\" , str ( batch_size )]) logger . info ( \"Mosec Server Configurations: %s \" , args ) return args def _start_controller ( self ): \"\"\"Subprocess to start controller program.\"\"\" self . _configs = vars ( parse_arguments ()) if not self . _server_shutdown : path = self . _configs [ \"path\" ] if exists ( path ): logger . info ( \"path already exists, try to remove it: %s \" , path ) rmtree ( path ) path = Path ( pkg_resources . resource_filename ( \"mosec\" , \"bin\" ), \"mosec\" ) # pylint: disable=consider-using-with self . _controller_process = subprocess . Popen ( [ path ] + self . _controller_args () ) self . register_daemon ( \"controller\" , self . _controller_process ) def _terminate ( self , signum , framestack ): logger . info ( \"[ %s ] terminating server [ %s ] ...\" , signum , framestack ) self . _server_shutdown = True @staticmethod def _clean_pools ( processes : List [ Union [ mp . Process , None ]], ) -> List [ Union [ mp . Process , None ]]: for i , process in enumerate ( processes ): if process is None or process . exitcode is not None : processes [ i ] = None return processes def _manage_coordinators ( self ): first = True while not self . _server_shutdown : for stage_id , ( w_cls , w_num , w_mbs , c_ctx , c_env ) in enumerate ( zip ( self . _worker_cls , self . _worker_num , self . _worker_mbs , self . _coordinator_ctx , self . _coordinator_env , ) ): # for every sequential stage self . _coordinator_pools [ stage_id ] = self . _clean_pools ( self . _coordinator_pools [ stage_id ] ) if all ( self . _coordinator_pools [ stage_id ]): # this stage is healthy continue if not first and not any ( self . _coordinator_pools [ stage_id ]): # this stage might contain bugs self . _terminate ( 1 , f \"all workers at stage { stage_id } exited;\" \" please check for bugs or socket connection issues\" , ) break stage = \"\" if stage_id == 0 : stage += STAGE_INGRESS if stage_id == len ( self . _worker_cls ) - 1 : stage += STAGE_EGRESS for worker_id in range ( w_num ): # for every worker in each stage if self . _coordinator_pools [ stage_id ][ worker_id ] is not None : continue coordinator_process = mp . get_context ( c_ctx ) . Process ( target = Coordinator , args = ( w_cls , w_mbs , stage , self . _coordinator_shutdown , self . _coordinator_shutdown_notify , self . _configs [ \"path\" ], stage_id + 1 , worker_id + 1 , self . ipc_wrapper , ), daemon = True , ) with env_var_context ( c_env , worker_id ): coordinator_process . start () self . _coordinator_pools [ stage_id ][ worker_id ] = coordinator_process first = False self . _check_daemon () sleep ( GUARD_CHECK_INTERVAL ) def _halt ( self ): \"\"\"Graceful shutdown.\"\"\" # notify coordinators for the shutdown self . _coordinator_shutdown_notify . set () # terminate controller first and wait for a graceful period if self . _controller_process : self . _controller_process . terminate () graceful_period = monotonic () + self . _configs [ \"timeout\" ] / 1000 while monotonic () < graceful_period : ctr_exitcode = self . _controller_process . poll () if ctr_exitcode is not None : # exited if ctr_exitcode : # on error logger . error ( \"mosec controller halted on error: %d \" , ctr_exitcode ) else : logger . info ( \"mosec controller halted normally\" ) break sleep ( 0.1 ) # shutdown coordinators self . _coordinator_shutdown . set () logger . info ( \"mosec server exited. see you.\" ) def register_daemon ( self , name : str , proc : mp . Process ): \"\"\"Register a daemon to be monitored. Args: name (str): the name of this daemon proc (mp.Process): the process handle of the daemon \"\"\" assert isinstance ( name , str ), \"daemon name should be a string\" assert isinstance ( proc , ( mp . Process , subprocess . Popen ) ), f \" { type ( proc ) } is not a process or subprocess\" self . _daemon [ name ] = proc def append_worker ( self , worker : Type [ Worker ], num : int = 1 , max_batch_size : int = 1 , start_method : str = \"spawn\" , env : Union [ None , List [ Dict [ str , str ]]] = None , ): \"\"\"Sequentially appends workers to the workflow pipeline. Arguments: worker: the class you inherit from `Worker` which implements the `forward` method num: the number of processes for parallel computing (>=1) max_batch_size: the maximum batch size allowed (>=1) start_method: the process starting method (\"spawn\" or \"fork\") env: the environment variables to set before starting the process \"\"\" self . _validate_arguments ( worker , num , max_batch_size , start_method , env ) self . _worker_cls . append ( worker ) self . _worker_num . append ( num ) self . _worker_mbs . append ( max_batch_size ) self . _coordinator_env . append ( env ) self . _coordinator_ctx . append ( start_method ) self . _coordinator_pools . append ([ None ] * num ) def run ( self ): \"\"\"Start the mosec model server.\"\"\" self . _validate_server () self . _start_controller () try : self . _manage_coordinators () # pylint: disable=broad-except except Exception : logger . error ( traceback . format_exc () . replace ( \" \\n \" , \" \" )) self . _halt ()","title":"IPC Wrapper"},{"location":"interface/#mosec.server.Server.__init__","text":"Initialize a MOSEC Server. Parameters: Name Type Description Default ipc_wrapper Optional [ Union [ IPCWrapper , partial ]] IPCWrapper class. Defaults to None. None Source code in mosec/server.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __init__ ( self , ipc_wrapper : Optional [ Union [ IPCWrapper , partial ]] = None , ): \"\"\"Initialize a MOSEC Server. Args: ipc_wrapper (Optional[Union[IPCWrapper, partial]], optional): IPCWrapper class. Defaults to None. \"\"\" self . ipc_wrapper = ipc_wrapper self . _worker_cls : List [ Type [ Worker ]] = [] self . _worker_num : List [ int ] = [] self . _worker_mbs : List [ int ] = [] self . _coordinator_env : List [ Union [ None , List [ Dict [ str , str ]]]] = [] self . _coordinator_ctx : List [ str ] = [] self . _coordinator_pools : List [ List [ Union [ mp . Process , None ]]] = [] self . _coordinator_shutdown : Event = mp . get_context ( \"spawn\" ) . Event () self . _coordinator_shutdown_notify : Event = mp . get_context ( \"spawn\" ) . Event () self . _controller_process : Optional [ subprocess . Popen ] = None self . _daemon : Dict [ str , Union [ subprocess . Popen , mp . Process ]] = {} self . _configs : dict = {} self . _server_shutdown : bool = False signal . signal ( signal . SIGTERM , self . _terminate ) signal . signal ( signal . SIGINT , self . _terminate )","title":"__init__()"},{"location":"interface/#mosec.server.Server.append_worker","text":"Sequentially appends workers to the workflow pipeline. Parameters: Name Type Description Default worker Type [ Worker ] the class you inherit from Worker which implements the forward method required num int the number of processes for parallel computing (>=1) 1 max_batch_size int the maximum batch size allowed (>=1) 1 start_method str the process starting method (\"spawn\" or \"fork\") 'spawn' env Union [None, List [ Dict [ str , str ]]] the environment variables to set before starting the process None Source code in mosec/server.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def append_worker ( self , worker : Type [ Worker ], num : int = 1 , max_batch_size : int = 1 , start_method : str = \"spawn\" , env : Union [ None , List [ Dict [ str , str ]]] = None , ): \"\"\"Sequentially appends workers to the workflow pipeline. Arguments: worker: the class you inherit from `Worker` which implements the `forward` method num: the number of processes for parallel computing (>=1) max_batch_size: the maximum batch size allowed (>=1) start_method: the process starting method (\"spawn\" or \"fork\") env: the environment variables to set before starting the process \"\"\" self . _validate_arguments ( worker , num , max_batch_size , start_method , env ) self . _worker_cls . append ( worker ) self . _worker_num . append ( num ) self . _worker_mbs . append ( max_batch_size ) self . _coordinator_env . append ( env ) self . _coordinator_ctx . append ( start_method ) self . _coordinator_pools . append ([ None ] * num )","title":"append_worker()"},{"location":"interface/#mosec.server.Server.register_daemon","text":"Register a daemon to be monitored. Parameters: Name Type Description Default name str the name of this daemon required proc mp . Process the process handle of the daemon required Source code in mosec/server.py 293 294 295 296 297 298 299 300 301 302 303 304 def register_daemon ( self , name : str , proc : mp . Process ): \"\"\"Register a daemon to be monitored. Args: name (str): the name of this daemon proc (mp.Process): the process handle of the daemon \"\"\" assert isinstance ( name , str ), \"daemon name should be a string\" assert isinstance ( proc , ( mp . Process , subprocess . Popen ) ), f \" { type ( proc ) } is not a process or subprocess\" self . _daemon [ name ] = proc","title":"register_daemon()"},{"location":"interface/#mosec.server.Server.run","text":"Start the mosec model server. Source code in mosec/server.py 332 333 334 335 336 337 338 339 340 341 def run ( self ): \"\"\"Start the mosec model server.\"\"\" self . _validate_server () self . _start_controller () try : self . _manage_coordinators () # pylint: disable=broad-except except Exception : logger . error ( traceback . format_exc () . replace ( \" \\n \" , \" \" )) self . _halt ()","title":"run()"},{"location":"interface/#mosec.server.env_var_context","text":"Manage the environment variables for a worker process. Source code in mosec/server.py 344 345 346 347 348 349 350 351 352 353 354 355 356 @contextlib . contextmanager def env_var_context ( env : Union [ None , List [ Dict [ str , str ]]], index : int ): \"\"\"Manage the environment variables for a worker process.\"\"\" default : Dict = {} try : if env is not None : for key , value in env [ index ] . items (): default [ key ] = os . getenv ( key , \"\" ) os . environ [ key ] = value yield None finally : for key , value in default . items (): os . environ [ key ] = value","title":"env_var_context()"},{"location":"interface/#mosec.worker","text":"MOSEC worker interface. This module provides the interface to define a worker with such behaviors initialize serialize/deserialize data to/from another worker serialize/deserialize data to/from the client side data processing","title":"worker"},{"location":"interface/#mosec.worker.Worker","text":"Bases: abc . ABC MOSEC worker interface. It provides default IPC (de)serialization methods, stores the worker meta data including its stage and maximum batch size, and leaves the forward method to be implemented by the users. By default, we use JSON encoding. But users are free to customize via simply overridding the deserialize method in the first stage (we term it as ingress stage) and/or the serialize method in the last stage (we term it as egress stage). For the encoding customization, there are many choices including MessagePack , Protocol Buffer and many other out-of-the-box protocols. Users can even define their own protocol and use it to manipulate the raw bytes! A naive customization can be found in this example .","title":"Worker"},{"location":"interface/#mosec.worker.Worker--note","text":"The \" same type \" mentioned below is applicable only when the stage disables batching. For a stage that enables batching , the worker 's forward should accept a list and output a list, where each element will follow the \" same type \" constraint. Source code in mosec/worker.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class Worker ( abc . ABC ): \"\"\"MOSEC worker interface. It provides default IPC (de)serialization methods, stores the worker meta data including its stage and maximum batch size, and leaves the `forward` method to be implemented by the users. By default, we use [JSON](https://www.json.org/) encoding. But users are free to customize via simply overridding the `deserialize` method in the **first** stage (we term it as _ingress_ stage) and/or the `serialize` method in the **last** stage (we term it as _egress_ stage). For the encoding customization, there are many choices including [MessagePack](https://msgpack.org/index.html), [Protocol Buffer](https://developers.google.com/protocol-buffers) and many other out-of-the-box protocols. Users can even define their own protocol and use it to manipulate the raw bytes! A naive customization can be found in [this example](https://mosecorg.github.io/mosec/example/pytorch/#sentiment-analysis). ###### Note > The \"_same type_\" mentioned below is applicable only when the stage disables batching. For a stage that [enables batching][mosec.server.Server--batching], the `worker`'s `forward` should accept a list and output a list, where each element will follow the \"_same type_\" constraint. \"\"\" # pylint: disable=no-self-use example : Any = None _worker_id : int = 0 def __init__ ( self ): \"\"\"Initialize the worker.\"\"\" self . _stage : str = \"\" self . _max_batch_size : int = 1 def serialize_ipc ( self , data ) -> bytes : \"\"\"Define IPC serialize method.\"\"\" return pickle . dumps ( data , protocol = pickle . HIGHEST_PROTOCOL ) def deserialize_ipc ( self , data ) -> Any : \"\"\"Define IPC deserialize method.\"\"\" return pickle . loads ( data ) @property def stage ( self ) -> str : \"\"\"Return the stage name.\"\"\" return self . _stage @stage . setter def stage ( self , stage ): self . _stage = stage @property def max_batch_size ( self ) -> int : \"\"\"Return the maximum batch size.\"\"\" return self . _max_batch_size @max_batch_size . setter def max_batch_size ( self , max_batch_size ): self . _max_batch_size = max_batch_size @property def worker_id ( self ) -> int : \"\"\"Return the id of this worker instance. This property returns the worker id in the range of [1, ... ,`num`] (`num` as defined [here][mosec.server.Server--multiprocess]) to differentiate workers in the same stage. \"\"\" return self . _worker_id def serialize ( self , data : Any ) -> bytes : \"\"\"Serialize method for the last stage (egress). No need to override this method by default, but overridable. Arguments: data: the [_*same type_][mosec.worker.Worker--note] as the output of the `forward` you implement Returns: the bytes you want to put into the response body Raises: ValueError: if the data cannot be serialized with JSON \"\"\" try : data_bytes = json . dumps ( data , indent = 2 ) . encode () except Exception as err : raise ValueError from err return data_bytes def deserialize ( self , data : bytes ) -> Any : \"\"\"Deserialize method for the first stage (ingress). No need to override this method by default, but overridable. Arguments: data: the raw bytes extracted from the request body Returns: the [_*same type_][mosec.worker.Worker--note] as the argument of the `forward` you implement Raises: DecodingError: if the data cannot be deserialized with JSON \"\"\" try : data_json = json . loads ( data ) if data else {} except Exception as err : raise DecodingError from err return data_json @abc . abstractmethod def forward ( self , data : Any ) -> Any : \"\"\"Model inference, data processing or computation logic. __Must be overridden__ by the subclass. The implementation should make sure: - (for a single-stage worker) - both the input and output follow the [_*same type_][mosec.worker.Worker--note] rule. - (for a multi-stage worker) - the input of the _ingress_ stage and the output of the _egress_ stage follow the [_*same type_][mosec.worker.Worker--note], while others should align with its adjacent stage's in/output. If any code in the `forward` needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the `__init__` method. \"\"\" raise NotImplementedError","title":"Note"},{"location":"interface/#mosec.worker.Worker.__init__","text":"Initialize the worker. Source code in mosec/worker.py 69 70 71 72 def __init__ ( self ): \"\"\"Initialize the worker.\"\"\" self . _stage : str = \"\" self . _max_batch_size : int = 1","title":"__init__()"},{"location":"interface/#mosec.worker.Worker.deserialize","text":"Deserialize method for the first stage (ingress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data bytes the raw bytes extracted from the request body required Returns: Type Description Any the *same type as the argument of Any the forward you implement Raises: Type Description DecodingError if the data cannot be deserialized with JSON Source code in mosec/worker.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def deserialize ( self , data : bytes ) -> Any : \"\"\"Deserialize method for the first stage (ingress). No need to override this method by default, but overridable. Arguments: data: the raw bytes extracted from the request body Returns: the [_*same type_][mosec.worker.Worker--note] as the argument of the `forward` you implement Raises: DecodingError: if the data cannot be deserialized with JSON \"\"\" try : data_json = json . loads ( data ) if data else {} except Exception as err : raise DecodingError from err return data_json","title":"deserialize()"},{"location":"interface/#mosec.worker.Worker.deserialize_ipc","text":"Define IPC deserialize method. Source code in mosec/worker.py 78 79 80 def deserialize_ipc ( self , data ) -> Any : \"\"\"Define IPC deserialize method.\"\"\" return pickle . loads ( data )","title":"deserialize_ipc()"},{"location":"interface/#mosec.worker.Worker.forward","text":"Model inference, data processing or computation logic. Must be overridden by the subclass. The implementation should make sure: (for a single-stage worker) both the input and output follow the *same type rule. (for a multi-stage worker) the input of the ingress stage and the output of the egress stage follow the *same type , while others should align with its adjacent stage's in/output. If any code in the forward needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the __init__ method. Source code in mosec/worker.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 @abc . abstractmethod def forward ( self , data : Any ) -> Any : \"\"\"Model inference, data processing or computation logic. __Must be overridden__ by the subclass. The implementation should make sure: - (for a single-stage worker) - both the input and output follow the [_*same type_][mosec.worker.Worker--note] rule. - (for a multi-stage worker) - the input of the _ingress_ stage and the output of the _egress_ stage follow the [_*same type_][mosec.worker.Worker--note], while others should align with its adjacent stage's in/output. If any code in the `forward` needs to access other resources (e.g. a model, a memory cache, etc.), the user should initialize these resources to be attributes of the class in the `__init__` method. \"\"\" raise NotImplementedError","title":"forward()"},{"location":"interface/#mosec.worker.Worker.max_batch_size","text":"Return the maximum batch size. Source code in mosec/worker.py 91 92 93 94 @property def max_batch_size ( self ) -> int : \"\"\"Return the maximum batch size.\"\"\" return self . _max_batch_size","title":"max_batch_size()"},{"location":"interface/#mosec.worker.Worker.serialize","text":"Serialize method for the last stage (egress). No need to override this method by default, but overridable. Parameters: Name Type Description Default data Any the *same type as the output of the forward you implement required Returns: Type Description bytes the bytes you want to put into the response body Raises: Type Description ValueError if the data cannot be serialized with JSON Source code in mosec/worker.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def serialize ( self , data : Any ) -> bytes : \"\"\"Serialize method for the last stage (egress). No need to override this method by default, but overridable. Arguments: data: the [_*same type_][mosec.worker.Worker--note] as the output of the `forward` you implement Returns: the bytes you want to put into the response body Raises: ValueError: if the data cannot be serialized with JSON \"\"\" try : data_bytes = json . dumps ( data , indent = 2 ) . encode () except Exception as err : raise ValueError from err return data_bytes","title":"serialize()"},{"location":"interface/#mosec.worker.Worker.serialize_ipc","text":"Define IPC serialize method. Source code in mosec/worker.py 74 75 76 def serialize_ipc ( self , data ) -> bytes : \"\"\"Define IPC serialize method.\"\"\" return pickle . dumps ( data , protocol = pickle . HIGHEST_PROTOCOL )","title":"serialize_ipc()"},{"location":"interface/#mosec.worker.Worker.stage","text":"Return the stage name. Source code in mosec/worker.py 82 83 84 85 @property def stage ( self ) -> str : \"\"\"Return the stage name.\"\"\" return self . _stage","title":"stage()"},{"location":"interface/#mosec.worker.Worker.worker_id","text":"Return the id of this worker instance. This property returns the worker id in the range of [1, ... , num ] ( num as defined here ) to differentiate workers in the same stage. Source code in mosec/worker.py 100 101 102 103 104 105 106 107 108 @property def worker_id ( self ) -> int : \"\"\"Return the id of this worker instance. This property returns the worker id in the range of [1, ... ,`num`] (`num` as defined [here][mosec.server.Server--multiprocess]) to differentiate workers in the same stage. \"\"\" return self . _worker_id","title":"worker_id()"},{"location":"interface/#mosec.errors","text":"Exceptions used in the Worker. Suppose the input dataflow of our model server is as follows: bytes --- deserialize (decoding) ---> data --- parse (validation) ---> valid data If the raw bytes cannot be successfully deserialized, the DecodingError is raised; if the decoded data cannot pass the validation check (usually implemented by users), the ValidationError should be raised.","title":"errors"},{"location":"interface/#mosec.errors.DecodingError","text":"Bases: Exception De-serialization error. The DecodingError should be raised in user-implemented codes when the de-serialization for the request bytes fails. This error will set the status code to HTTP 400 in the response. Source code in mosec/errors.py 28 29 30 31 32 33 34 35 36 class DecodingError ( Exception ): \"\"\"De-serialization error. The `DecodingError` should be raised in user-implemented codes when the de-serialization for the request bytes fails. This error will set the status code to [HTTP 400](\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400) in the response. \"\"\"","title":"DecodingError"},{"location":"interface/#mosec.errors.ValidationError","text":"Bases: Exception Request data validation error. The ValidationError should be raised in user-implemented codes, where the validation for the input data fails. Usually, it should be put after the data de-serialization, which converts the raw bytes into structured data. This error will set the status code to HTTP 422 in the response. Source code in mosec/errors.py 39 40 41 42 43 44 45 46 47 48 class ValidationError ( Exception ): \"\"\"Request data validation error. The `ValidationError` should be raised in user-implemented codes, where the validation for the input data fails. Usually, it should be put after the data de-serialization, which converts the raw bytes into structured data. This error will set the status code to [HTTP 422](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422) in the response. \"\"\"","title":"ValidationError"},{"location":"interface/#mosec.plugins","text":"Provide useful tools to extend MOSEC.","title":"plugins"},{"location":"interface/#mosec.plugins.PlasmaShmWrapper","text":"Bases: IPCWrapper Shared memory wrapper using pyarrow Plasma. This public class is an example implementation of the IPCWrapper . It utilizes pyarrow.plasma as the in-memory object store for potentially more efficient data transfer. Source code in mosec/plugins/plasma_shm.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class PlasmaShmWrapper ( IPCWrapper ): \"\"\"Shared memory wrapper using `pyarrow` Plasma. This public class is an example implementation of the `IPCWrapper`. It utilizes `pyarrow.plasma` as the in-memory object store for potentially more efficient data transfer. \"\"\" def __init__ ( self , shm_path : str ) -> None : \"\"\"Initialize the IPC Wrapper as a plasma client. Args: shm_path (str): path of the plasma server. \"\"\" self . client = plasma . connect ( shm_path ) def _put_plasma ( self , data : List [ bytes ]) -> List [ plasma . ObjectID ]: \"\"\"Batch put into plasma memory store.\"\"\" return [ self . client . put ( x ) for x in data ] def _get_plasma ( self , object_ids : List [ plasma . ObjectID ]) -> List [ bytes ]: \"\"\"Batch get from plasma memory store.\"\"\" objects = self . client . get ( object_ids ) self . client . delete ( object_ids ) return objects def put ( self , data : List [ bytes ]) -> List [ bytes ]: \"\"\"Save data to the plasma memory store and return the ID.\"\"\" object_ids = self . _put_plasma ( data ) return [ id . binary () for id in object_ids ] def get ( self , ids : List [ bytes ]) -> List [ bytes ]: \"\"\"Get data from the plasma memory store by ID.\"\"\" object_ids = [ plasma . ObjectID ( id ) for id in ids ] return self . _get_plasma ( object_ids )","title":"PlasmaShmWrapper"},{"location":"interface/#mosec.plugins.plasma_shm.PlasmaShmWrapper.__init__","text":"Initialize the IPC Wrapper as a plasma client. Parameters: Name Type Description Default shm_path str path of the plasma server. required Source code in mosec/plugins/plasma_shm.py 47 48 49 50 51 52 53 def __init__ ( self , shm_path : str ) -> None : \"\"\"Initialize the IPC Wrapper as a plasma client. Args: shm_path (str): path of the plasma server. \"\"\" self . client = plasma . connect ( shm_path )","title":"__init__()"},{"location":"interface/#mosec.plugins.plasma_shm.PlasmaShmWrapper.get","text":"Get data from the plasma memory store by ID. Source code in mosec/plugins/plasma_shm.py 70 71 72 73 def get ( self , ids : List [ bytes ]) -> List [ bytes ]: \"\"\"Get data from the plasma memory store by ID.\"\"\" object_ids = [ plasma . ObjectID ( id ) for id in ids ] return self . _get_plasma ( object_ids )","title":"get()"},{"location":"interface/#mosec.plugins.plasma_shm.PlasmaShmWrapper.put","text":"Save data to the plasma memory store and return the ID. Source code in mosec/plugins/plasma_shm.py 65 66 67 68 def put ( self , data : List [ bytes ]) -> List [ bytes ]: \"\"\"Save data to the plasma memory store and return the ID.\"\"\" object_ids = self . _put_plasma ( data ) return [ id . binary () for id in object_ids ]","title":"put()"},{"location":"design/","text":"We will explain the detail of our designs in this section.","title":"Overview"},{"location":"example/","text":"We provide examples across different ML frameworks and for various tasks in this section. Get started \u00a4 All the examples in this section are self-contained and tested. Feel free to grab one and run: python model_server.py To test the server, we use httpie and httpx by default. You can have other choices but if you want to install them: pip install httpie httpx","title":"Overview"},{"location":"example/#get-started","text":"All the examples in this section are self-contained and tested. Feel free to grab one and run: python model_server.py To test the server, we use httpie and httpx by default. You can have other choices but if you want to install them: pip install httpie httpx","title":"Get started"},{"location":"example/echo/","text":"An echo server is usually the very first server you wanna implement to get familiar with the framework. This server sleeps for a given period and return. It is a simple illustration of how multi-stage workload is implemented. It also shows how to write a simple validation for input data. The default JSON protocol will be used since the (de)serialization methods are not overridden in this demo. In particular, the input data of Preprocess 's forward is a dictionary decoded by JSON from the request body's bytes; and the output dictionary of Postprocess 's forward will be JSON-encoded as a mirrored process. echo.py \u00a4 # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging import time from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) class Preprocess ( Worker ): def forward ( self , data : dict ) -> float : logger . debug ( f \"pre received { data } \" ) # Customized, simple input validation try : time = float ( data [ \"time\" ]) except KeyError as err : raise ValidationError ( f \"cannot find key { err } \" ) return time class Inference ( Worker ): def forward ( self , data : float ) -> float : logger . info ( f \"sleeping for { data } seconds\" ) time . sleep ( data ) return data class Postprocess ( Worker ): def forward ( self , data : float ) -> dict : logger . debug ( f \"post received { data } \" ) return { \"msg\" : f \"sleep { data } seconds\" } if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess ) server . append_worker ( Inference ) server . append_worker ( Postprocess ) server . run () Start \u00a4 python echo.py Test \u00a4 http :8000/inference time=1.5","title":"Echo"},{"location":"example/echo/#echopy","text":"# Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging import time from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) class Preprocess ( Worker ): def forward ( self , data : dict ) -> float : logger . debug ( f \"pre received { data } \" ) # Customized, simple input validation try : time = float ( data [ \"time\" ]) except KeyError as err : raise ValidationError ( f \"cannot find key { err } \" ) return time class Inference ( Worker ): def forward ( self , data : float ) -> float : logger . info ( f \"sleeping for { data } seconds\" ) time . sleep ( data ) return data class Postprocess ( Worker ): def forward ( self , data : float ) -> dict : logger . debug ( f \"post received { data } \" ) return { \"msg\" : f \"sleep { data } seconds\" } if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess ) server . append_worker ( Inference ) server . append_worker ( Postprocess ) server . run ()","title":"echo.py"},{"location":"example/echo/#start","text":"python echo.py","title":"Start"},{"location":"example/echo/#test","text":"http :8000/inference time=1.5","title":"Test"},{"location":"example/env/","text":"This is an example demonstrating how to give different worker processes customized environment variables to control things like GPU device allocation, etc. Assume your machine has 4 GPUs, and you hope to deploy your model to all of them to handle inference requests in parallel, maximizing your service's throughput. With MOSEC, we provide parallel workers with customized environment variables to satisfy the needs. As shown in the codes below, we can define our inference worker together with a list of environment variable dictionaries, each of which will be passed to the corresponding worker process. For example, if we set CUDA_VISIBLE_DEVICES to 0-3 , (the same copy of) our model will be deployed on 4 different GPUs and be queried in parallel, largely improving the system's throughput. You could verify this either from the server logs or the client response. custom_env.py \u00a4 # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging import os from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) class Inference ( Worker ): def __init__ ( self ): super () . __init__ () # initialize your models here and allocate dedicated device to it device = os . environ [ \"CUDA_VISIBLE_DEVICES\" ] logger . info ( f \"Initializing model on device= { device } \" ) def forward ( self , _ : dict ) -> dict : device = os . environ [ \"CUDA_VISIBLE_DEVICES\" ] # NOTE self.worker_id is 1-indexed logger . info ( f \"Worker= { self . worker_id } on device= { device } is processing...\" ) return { \"device\" : device } if __name__ == \"__main__\" : num_device = 4 def _get_cuda_device ( cid : int ) -> dict : return { \"CUDA_VISIBLE_DEVICES\" : str ( cid )} server = Server () server . append_worker ( Inference , num = num_device , env = [ _get_cuda_device ( x ) for x in range ( num_device )] ) server . run () Start \u00a4 python custom_env.py Test \u00a4 curl -X POST http://127.0.0.1:8000/inference -d '{\"dummy\": 0}'","title":"Environments"},{"location":"example/env/#custom_envpy","text":"# Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging import os from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) class Inference ( Worker ): def __init__ ( self ): super () . __init__ () # initialize your models here and allocate dedicated device to it device = os . environ [ \"CUDA_VISIBLE_DEVICES\" ] logger . info ( f \"Initializing model on device= { device } \" ) def forward ( self , _ : dict ) -> dict : device = os . environ [ \"CUDA_VISIBLE_DEVICES\" ] # NOTE self.worker_id is 1-indexed logger . info ( f \"Worker= { self . worker_id } on device= { device } is processing...\" ) return { \"device\" : device } if __name__ == \"__main__\" : num_device = 4 def _get_cuda_device ( cid : int ) -> dict : return { \"CUDA_VISIBLE_DEVICES\" : str ( cid )} server = Server () server . append_worker ( Inference , num = num_device , env = [ _get_cuda_device ( x ) for x in range ( num_device )] ) server . run ()","title":"custom_env.py"},{"location":"example/env/#start","text":"python custom_env.py","title":"Start"},{"location":"example/env/#test","text":"curl -X POST http://127.0.0.1:8000/inference -d '{\"dummy\": 0}'","title":"Test"},{"location":"example/ipc/","text":"This is an example demonstrating how you can enable the plasma shared memory store or customize your own IPC wrapper. Mosec's multi-stage pipeline requires the output data from the previous stage to be transferred to the next stage across python processes. This is coordinated via Unix domain socket between every Python worker process from all stages and the Rust controller process. By default, we serialize the data and directly transfer the bytes over the socket. However, users may find wrapping this IPC useful or more efficient for specific use cases. Therefore, we provide the mosec.plugins.IPCWrapper interface and an example implementation PlasmaShmWrapper based on pyarrow.plasma . The additional subprocess can be registered as a daemon thus it will be checked by mosec regularly and trigger graceful shutdown when the daemon exits. plasma_shm_ipc.py \u00a4 # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from functools import partial from pyarrow import plasma # type: ignore from mosec import Server , Worker from mosec.errors import ValidationError from mosec.plugins import PlasmaShmWrapper class DataProducer ( Worker ): def forward ( self , data : dict ) -> bytes : try : data_bytes = b \"a\" * int ( data [ \"size\" ]) except KeyError as err : raise ValidationError ( err ) return data_bytes class DataConsumer ( Worker ): def forward ( self , data : bytes ) -> dict : return { \"ipc test data length\" : len ( data )} if __name__ == \"__main__\" : \"\"\" We start a subprocess for the plasma server, and pass the path to the plasma client which serves as the shm wrapper. We also register the plasma server process as a daemon, so that when it exits the service is able to gracefully shutdown and restarted by the orchestrator. \"\"\" # 200 Mb store, adjust the size according to your requirement with plasma . start_plasma_store ( plasma_store_memory = 200 * 1000 * 1000 ) as ( shm_path , shm_process , ): server = Server ( ipc_wrapper = partial ( # defer the wrapper init to worker processes PlasmaShmWrapper , shm_path = shm_path , ) ) server . register_daemon ( \"plasma_server\" , shm_process ) server . append_worker ( DataProducer , num = 2 ) server . append_worker ( DataConsumer , num = 2 ) server . run () Start \u00a4 python plasma_shm_ipc.py Test \u00a4 curl -X POST http://127.0.0.1:8000/inference -d '{\"size\": 100}'","title":"Plugins - IPCWrapper"},{"location":"example/ipc/#plasma_shm_ipcpy","text":"# Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from functools import partial from pyarrow import plasma # type: ignore from mosec import Server , Worker from mosec.errors import ValidationError from mosec.plugins import PlasmaShmWrapper class DataProducer ( Worker ): def forward ( self , data : dict ) -> bytes : try : data_bytes = b \"a\" * int ( data [ \"size\" ]) except KeyError as err : raise ValidationError ( err ) return data_bytes class DataConsumer ( Worker ): def forward ( self , data : bytes ) -> dict : return { \"ipc test data length\" : len ( data )} if __name__ == \"__main__\" : \"\"\" We start a subprocess for the plasma server, and pass the path to the plasma client which serves as the shm wrapper. We also register the plasma server process as a daemon, so that when it exits the service is able to gracefully shutdown and restarted by the orchestrator. \"\"\" # 200 Mb store, adjust the size according to your requirement with plasma . start_plasma_store ( plasma_store_memory = 200 * 1000 * 1000 ) as ( shm_path , shm_process , ): server = Server ( ipc_wrapper = partial ( # defer the wrapper init to worker processes PlasmaShmWrapper , shm_path = shm_path , ) ) server . register_daemon ( \"plasma_server\" , shm_process ) server . append_worker ( DataProducer , num = 2 ) server . append_worker ( DataConsumer , num = 2 ) server . run ()","title":"plasma_shm_ipc.py"},{"location":"example/ipc/#start","text":"python plasma_shm_ipc.py","title":"Start"},{"location":"example/ipc/#test","text":"curl -X POST http://127.0.0.1:8000/inference -d '{\"size\": 100}'","title":"Test"},{"location":"example/metric/","text":"This is an example demonstrating how to add your customized Python side Prometheus metrics. Mosec already has the Rust side metrics, including: throughput for the inference endpoint duration for each stage (including the IPC time) batch size (only for the max_batch_size > 1 workers) number of remaining tasks to be processed If you need to monitor more details about the inference process, you can add some Python side metrics. E.g., the inference result distribution, the duration of some CPU-bound or GPU-bound processing, the IPC time (get from rust_step_duration - python_step_duration ). This example has a simple WSGI app as the monitoring metrics service. In each worker process, the Counter will collect the inference results and export them to the metrics service. For the inference part, it parses the batch data and compares them with the average value. For more information about the multiprocess mode for the metrics, check the Prometheus doc . python_side_metrics.py \u00a4 # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging import os import pathlib import tempfile import threading from typing import List from wsgiref.simple_server import make_server from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) # check the PROMETHEUS_MULTIPROC_DIR environment variable before import Prometheus if not os . environ . get ( \"PROMETHEUS_MULTIPROC_DIR\" ): metric_dir_path = os . path . join ( tempfile . gettempdir (), \"prometheus_multiproc_dir\" ) pathlib . Path ( metric_dir_path ) . mkdir ( parents = True , exist_ok = True ) os . environ [ \"PROMETHEUS_MULTIPROC_DIR\" ] = metric_dir_path from prometheus_client import ( # type: ignore CONTENT_TYPE_LATEST , CollectorRegistry , Counter , generate_latest , multiprocess , ) metric_registry = CollectorRegistry () multiprocess . MultiProcessCollector ( metric_registry ) counter = Counter ( \"inference_result\" , \"statistic of result\" , ( \"status\" , \"worker_id\" )) def metric_app ( environ , start_response ): data = generate_latest ( metric_registry ) start_response ( \"200 OK\" , [( \"Content-Type\" , CONTENT_TYPE_LATEST ), ( \"Content-Length\" , str ( len ( data )))], ) return iter ([ data ]) def metric_service ( host = \"\" , port = 8080 ): with make_server ( host , port , metric_app ) as httpd : httpd . serve_forever () class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . worker_id = str ( self . worker_id ) def deserialize ( self , data : bytes ) -> int : json_data = super () . deserialize ( data ) try : res = int ( json_data . get ( \"num\" )) except Exception as err : raise ValidationError ( err ) return res def forward ( self , data : List [ int ]) -> List [ bool ]: avg = sum ( data ) / len ( data ) ans = [ x >= avg for x in data ] counter . labels ( status = \"true\" , worker_id = self . worker_id ) . inc ( sum ( ans )) counter . labels ( status = \"false\" , worker_id = self . worker_id ) . inc ( len ( ans ) - sum ( ans ) ) return ans if __name__ == \"__main__\" : # Run the metrics server in another thread. metric_thread = threading . Thread ( target = metric_service , daemon = True ) metric_thread . start () # Run the inference server server = Server () server . append_worker ( Inference , num = 2 , max_batch_size = 8 ) server . run () Start \u00a4 python python_side_metrics.py Test \u00a4 http POST :8000/inference num=1 Check the Python side metrics \u00a4 http :8080 Check the Rust side metrics \u00a4 http :8000/metrics","title":"Metrics"},{"location":"example/metric/#python_side_metricspy","text":"# Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging import os import pathlib import tempfile import threading from typing import List from wsgiref.simple_server import make_server from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) # check the PROMETHEUS_MULTIPROC_DIR environment variable before import Prometheus if not os . environ . get ( \"PROMETHEUS_MULTIPROC_DIR\" ): metric_dir_path = os . path . join ( tempfile . gettempdir (), \"prometheus_multiproc_dir\" ) pathlib . Path ( metric_dir_path ) . mkdir ( parents = True , exist_ok = True ) os . environ [ \"PROMETHEUS_MULTIPROC_DIR\" ] = metric_dir_path from prometheus_client import ( # type: ignore CONTENT_TYPE_LATEST , CollectorRegistry , Counter , generate_latest , multiprocess , ) metric_registry = CollectorRegistry () multiprocess . MultiProcessCollector ( metric_registry ) counter = Counter ( \"inference_result\" , \"statistic of result\" , ( \"status\" , \"worker_id\" )) def metric_app ( environ , start_response ): data = generate_latest ( metric_registry ) start_response ( \"200 OK\" , [( \"Content-Type\" , CONTENT_TYPE_LATEST ), ( \"Content-Length\" , str ( len ( data )))], ) return iter ([ data ]) def metric_service ( host = \"\" , port = 8080 ): with make_server ( host , port , metric_app ) as httpd : httpd . serve_forever () class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . worker_id = str ( self . worker_id ) def deserialize ( self , data : bytes ) -> int : json_data = super () . deserialize ( data ) try : res = int ( json_data . get ( \"num\" )) except Exception as err : raise ValidationError ( err ) return res def forward ( self , data : List [ int ]) -> List [ bool ]: avg = sum ( data ) / len ( data ) ans = [ x >= avg for x in data ] counter . labels ( status = \"true\" , worker_id = self . worker_id ) . inc ( sum ( ans )) counter . labels ( status = \"false\" , worker_id = self . worker_id ) . inc ( len ( ans ) - sum ( ans ) ) return ans if __name__ == \"__main__\" : # Run the metrics server in another thread. metric_thread = threading . Thread ( target = metric_service , daemon = True ) metric_thread . start () # Run the inference server server = Server () server . append_worker ( Inference , num = 2 , max_batch_size = 8 ) server . run ()","title":"python_side_metrics.py"},{"location":"example/metric/#start","text":"python python_side_metrics.py","title":"Start"},{"location":"example/metric/#test","text":"http POST :8000/inference num=1","title":"Test"},{"location":"example/metric/#check-the-python-side-metrics","text":"http :8080","title":"Check the Python side metrics"},{"location":"example/metric/#check-the-rust-side-metrics","text":"http :8000/metrics","title":"Check the Rust side metrics"},{"location":"example/pytorch/","text":"Here are some out-of-the-box model servers powered by mosec for PyTorch users. We use the version 1.9.0 in the following examples. Natural Language Processing \u00a4 Natural language processing model servers usually receive text data and make predictions ranging from text classification, question answering to translation and text generation. Sentiment Analysis \u00a4 This server receives a string and predicts how positive its content is. We build the model server based on Transformers of version 4.11.0. We show how to customize the deserialize method of the ingress stage ( Preprocess ) and the serialize method of the egress stage ( Inference ). In this way, we can enjoy the high flexibility, directly reading data bytes from request body and writing the results into response body. Note that in a stage that enables batching (e.g. Inference in this example), its worker's forward method deals with a list of data, while its serialize and deserialize methods only need to manipulate individual datum. Server \u00a4 python distil_bert_sentiment.py distil_bert_sentiment.py # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging from typing import List , TypeVar import torch # type: ignore from transformers import ( # type: ignore AutoModelForSequenceClassification , AutoTokenizer , ) from mosec import Server , Worker T = TypeVar ( \"T\" ) logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 32 class Preprocess ( Worker ): def __init__ ( self ): super () . __init__ () self . tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased-finetuned-sst-2-english\" ) def deserialize ( self , data : bytes ) -> str : # Override `deserialize` for the *first* stage; # `data` is the raw bytes from the request body return data . decode () def forward ( self , data : str ) -> T : tokens = self . tokenizer . encode ( data , add_special_tokens = True ) return tokens class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = AutoModelForSequenceClassification . from_pretrained ( \"distilbert-base-uncased-finetuned-sst-2-english\" ) self . model . eval () self . model . to ( self . device ) # Overwrite self.example for warmup self . example = [ [ 101 , 2023 , 2003 , 1037 , 8403 , 4937 , 999 , 102 ] * 5 # make sentence longer ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ T ]) -> List [ str ]: tensors = [ torch . tensor ( token ) for token in data ] with torch . no_grad (): result = self . model ( torch . nn . utils . rnn . pad_sequence ( tensors , batch_first = True ) . to ( self . device ) )[ 0 ] scores = result . softmax ( dim = 1 ) . cpu () . tolist () return [ f \"positive= { p } \" for ( _ , p ) in scores ] def serialize ( self , data : str ) -> bytes : # Override `serialize` for the *last* stage; # `data` is the string from the `forward` output return data . encode () if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess ) server . append_worker ( Inference , max_batch_size = INFERENCE_BATCH_SIZE ) server . run () Client \u00a4 curl -X POST http://127.0.0.1:8000/inference -d 'i bought this product for many times, highly recommend' Computer Vision \u00a4 Computer vision model servers usually receive images or links to the images (downloading from the link becomes an I/O workload then), feed the preprocessed image data into the model and extract information like categories, bounding boxes and pixel labels as results. Image Recognition \u00a4 This server receives an image and classify it according to the ImageNet categorization. We specifically use ResNet as an image classifier and build a model service based on it. Nevertheless, this file serves as the starter code for any kind of image recognition model server. We enable multiprocessing for Preprocess stage, so that it can produce enough tasks for Inference stage to do batch inference , which better exploits the GPU computing power. More interestingly, we also started multiple model by setting the number of worker for Inference stage to 2. This is because a single model hardly fully occupy the GPU memory or utilization. Multiple models running on the same device in parallel can further increase our service throughput. When instantiating the Server , we enable plasma_shm , which utilizes the pyarrow.plasma as a shared memory data store for IPC. This could benefit the data transfer, especially when the data is large (preprocessed image data in this case), since its implementation uses nogil to escape the limitation of GIL. Note that you need to use pip install -U \"mosec[shm]\" to install necessary dependencies. We also demonstrate how to customized validation on the data content through this example. In the forward method of the Preprocess worker, we firstly check the key of the input, then try to decode the str and load it into array. If any of these steps fails, we raise the ValidationError . The status will be finally returned to our clients as HTTP 422 . Server \u00a4 python resnet50_server.py resnet50_server.py # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import base64 import logging from typing import List from urllib.request import urlretrieve import cv2 # type: ignore import numpy as np # type: ignore import torch # type: ignore import torchvision # type: ignore from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 16 class Preprocess ( Worker ): def forward ( self , req : dict ) -> np . ndarray : # Customized validation for input key and field content; raise # ValidationError so that the client can get 422 as http status try : image = req [ \"image\" ] im = np . frombuffer ( base64 . b64decode ( image ), np . uint8 ) im = cv2 . imdecode ( im , cv2 . IMREAD_COLOR )[:, :, :: - 1 ] # bgr -> rgb except KeyError as err : raise ValidationError ( f \"cannot find key { err } \" ) except Exception as err : raise ValidationError ( f \"cannot decode as image data: { err } \" ) im = cv2 . resize ( im , ( 256 , 256 )) crop_im = ( im [ 16 : 16 + 224 , 16 : 16 + 224 ] . astype ( np . float32 ) / 255 ) # center crop crop_im -= [ 0.485 , 0.456 , 0.406 ] crop_im /= [ 0.229 , 0.224 , 0.225 ] crop_im = np . transpose ( crop_im , ( 2 , 0 , 1 )) return crop_im class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = torchvision . models . resnet50 ( pretrained = True ) self . model . eval () self . model . to ( self . device ) # Overwrite self.example for warmup self . example = [ np . zeros (( 3 , 244 , 244 ), dtype = np . float32 ) ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ np . ndarray ]) -> List [ int ]: logger . info ( f \"processing batch with size: { len ( data ) } \" ) with torch . no_grad (): batch = torch . stack ([ torch . tensor ( arr , device = self . device ) for arr in data ]) output = self . model ( batch ) top1 = torch . argmax ( output , dim = 1 ) return top1 . cpu () . tolist () class Postprocess ( Worker ): def __init__ ( self ): super () . __init__ () logger . info ( \"loading categories file...\" ) local_filename , _ = urlretrieve ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" ) with open ( local_filename ) as f : self . categories = list ( map ( lambda x : x . strip (), f . readlines ())) def forward ( self , data : int ) -> dict : return { \"category\" : self . categories [ data ]} if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess , num = 4 ) server . append_worker ( Inference , num = 2 , max_batch_size = INFERENCE_BATCH_SIZE ) server . append_worker ( Postprocess , num = 1 ) server . run () Client \u00a4 python resnet50_client.py resnet50_client.py # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import base64 import httpx # type: ignore dog_bytes = httpx . get ( \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\" ) . content prediction = httpx . post ( \"http://localhost:8000/inference\" , json = { \"image\" : base64 . b64encode ( dog_bytes ) . decode ()}, ) if prediction . status_code == 200 : print ( prediction . json ()) else : print ( prediction . status_code , prediction . content )","title":"PyTorch"},{"location":"example/pytorch/#natural-language-processing","text":"Natural language processing model servers usually receive text data and make predictions ranging from text classification, question answering to translation and text generation.","title":"Natural Language Processing"},{"location":"example/pytorch/#sentiment-analysis","text":"This server receives a string and predicts how positive its content is. We build the model server based on Transformers of version 4.11.0. We show how to customize the deserialize method of the ingress stage ( Preprocess ) and the serialize method of the egress stage ( Inference ). In this way, we can enjoy the high flexibility, directly reading data bytes from request body and writing the results into response body. Note that in a stage that enables batching (e.g. Inference in this example), its worker's forward method deals with a list of data, while its serialize and deserialize methods only need to manipulate individual datum.","title":"Sentiment Analysis"},{"location":"example/pytorch/#server","text":"python distil_bert_sentiment.py distil_bert_sentiment.py # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging from typing import List , TypeVar import torch # type: ignore from transformers import ( # type: ignore AutoModelForSequenceClassification , AutoTokenizer , ) from mosec import Server , Worker T = TypeVar ( \"T\" ) logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 32 class Preprocess ( Worker ): def __init__ ( self ): super () . __init__ () self . tokenizer = AutoTokenizer . from_pretrained ( \"distilbert-base-uncased-finetuned-sst-2-english\" ) def deserialize ( self , data : bytes ) -> str : # Override `deserialize` for the *first* stage; # `data` is the raw bytes from the request body return data . decode () def forward ( self , data : str ) -> T : tokens = self . tokenizer . encode ( data , add_special_tokens = True ) return tokens class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = AutoModelForSequenceClassification . from_pretrained ( \"distilbert-base-uncased-finetuned-sst-2-english\" ) self . model . eval () self . model . to ( self . device ) # Overwrite self.example for warmup self . example = [ [ 101 , 2023 , 2003 , 1037 , 8403 , 4937 , 999 , 102 ] * 5 # make sentence longer ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ T ]) -> List [ str ]: tensors = [ torch . tensor ( token ) for token in data ] with torch . no_grad (): result = self . model ( torch . nn . utils . rnn . pad_sequence ( tensors , batch_first = True ) . to ( self . device ) )[ 0 ] scores = result . softmax ( dim = 1 ) . cpu () . tolist () return [ f \"positive= { p } \" for ( _ , p ) in scores ] def serialize ( self , data : str ) -> bytes : # Override `serialize` for the *last* stage; # `data` is the string from the `forward` output return data . encode () if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess ) server . append_worker ( Inference , max_batch_size = INFERENCE_BATCH_SIZE ) server . run ()","title":"Server"},{"location":"example/pytorch/#client","text":"curl -X POST http://127.0.0.1:8000/inference -d 'i bought this product for many times, highly recommend'","title":"Client"},{"location":"example/pytorch/#computer-vision","text":"Computer vision model servers usually receive images or links to the images (downloading from the link becomes an I/O workload then), feed the preprocessed image data into the model and extract information like categories, bounding boxes and pixel labels as results.","title":"Computer Vision"},{"location":"example/pytorch/#image-recognition","text":"This server receives an image and classify it according to the ImageNet categorization. We specifically use ResNet as an image classifier and build a model service based on it. Nevertheless, this file serves as the starter code for any kind of image recognition model server. We enable multiprocessing for Preprocess stage, so that it can produce enough tasks for Inference stage to do batch inference , which better exploits the GPU computing power. More interestingly, we also started multiple model by setting the number of worker for Inference stage to 2. This is because a single model hardly fully occupy the GPU memory or utilization. Multiple models running on the same device in parallel can further increase our service throughput. When instantiating the Server , we enable plasma_shm , which utilizes the pyarrow.plasma as a shared memory data store for IPC. This could benefit the data transfer, especially when the data is large (preprocessed image data in this case), since its implementation uses nogil to escape the limitation of GIL. Note that you need to use pip install -U \"mosec[shm]\" to install necessary dependencies. We also demonstrate how to customized validation on the data content through this example. In the forward method of the Preprocess worker, we firstly check the key of the input, then try to decode the str and load it into array. If any of these steps fails, we raise the ValidationError . The status will be finally returned to our clients as HTTP 422 .","title":"Image Recognition"},{"location":"example/pytorch/#server_1","text":"python resnet50_server.py resnet50_server.py # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import base64 import logging from typing import List from urllib.request import urlretrieve import cv2 # type: ignore import numpy as np # type: ignore import torch # type: ignore import torchvision # type: ignore from mosec import Server , Worker from mosec.errors import ValidationError logger = logging . getLogger () logger . setLevel ( logging . DEBUG ) formatter = logging . Formatter ( \" %(asctime)s - %(process)d - %(levelname)s - %(filename)s : %(lineno)s - %(message)s \" ) sh = logging . StreamHandler () sh . setFormatter ( formatter ) logger . addHandler ( sh ) INFERENCE_BATCH_SIZE = 16 class Preprocess ( Worker ): def forward ( self , req : dict ) -> np . ndarray : # Customized validation for input key and field content; raise # ValidationError so that the client can get 422 as http status try : image = req [ \"image\" ] im = np . frombuffer ( base64 . b64decode ( image ), np . uint8 ) im = cv2 . imdecode ( im , cv2 . IMREAD_COLOR )[:, :, :: - 1 ] # bgr -> rgb except KeyError as err : raise ValidationError ( f \"cannot find key { err } \" ) except Exception as err : raise ValidationError ( f \"cannot decode as image data: { err } \" ) im = cv2 . resize ( im , ( 256 , 256 )) crop_im = ( im [ 16 : 16 + 224 , 16 : 16 + 224 ] . astype ( np . float32 ) / 255 ) # center crop crop_im -= [ 0.485 , 0.456 , 0.406 ] crop_im /= [ 0.229 , 0.224 , 0.225 ] crop_im = np . transpose ( crop_im , ( 2 , 0 , 1 )) return crop_im class Inference ( Worker ): def __init__ ( self ): super () . __init__ () self . device = ( torch . device ( \"cuda\" ) if torch . cuda . is_available () else torch . device ( \"cpu\" ) ) logger . info ( f \"using computing device: { self . device } \" ) self . model = torchvision . models . resnet50 ( pretrained = True ) self . model . eval () self . model . to ( self . device ) # Overwrite self.example for warmup self . example = [ np . zeros (( 3 , 244 , 244 ), dtype = np . float32 ) ] * INFERENCE_BATCH_SIZE def forward ( self , data : List [ np . ndarray ]) -> List [ int ]: logger . info ( f \"processing batch with size: { len ( data ) } \" ) with torch . no_grad (): batch = torch . stack ([ torch . tensor ( arr , device = self . device ) for arr in data ]) output = self . model ( batch ) top1 = torch . argmax ( output , dim = 1 ) return top1 . cpu () . tolist () class Postprocess ( Worker ): def __init__ ( self ): super () . __init__ () logger . info ( \"loading categories file...\" ) local_filename , _ = urlretrieve ( \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\" ) with open ( local_filename ) as f : self . categories = list ( map ( lambda x : x . strip (), f . readlines ())) def forward ( self , data : int ) -> dict : return { \"category\" : self . categories [ data ]} if __name__ == \"__main__\" : server = Server () server . append_worker ( Preprocess , num = 4 ) server . append_worker ( Inference , num = 2 , max_batch_size = INFERENCE_BATCH_SIZE ) server . append_worker ( Postprocess , num = 1 ) server . run ()","title":"Server"},{"location":"example/pytorch/#client_1","text":"python resnet50_client.py resnet50_client.py # Copyright 2022 MOSEC Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import base64 import httpx # type: ignore dog_bytes = httpx . get ( \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\" ) . content prediction = httpx . post ( \"http://localhost:8000/inference\" , json = { \"image\" : base64 . b64encode ( dog_bytes ) . decode ()}, ) if prediction . status_code == 200 : print ( prediction . json ()) else : print ( prediction . status_code , prediction . content )","title":"Client"}]}